I"îæ<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p>In <a href="/2021/12/22/Part1_DMKDE.html">Part 1</a> we checked at the key mathematical and statistical concepts that could allow us to talk about Kernel Density Estimation using Density Matrices. In Part 2, we would like to describe how to use the algorithm already implemented in Python <a href="https://github.com/fagonzalezo/qmc">here</a> by Professor <a href="https://dis.unal.edu.co/~fgonza/">Fabio Gonzalez</a> and his research group <a href="http://www.ingenieria.unal.edu.co/mindlab/">MindLab</a> at Universidad Nacional de Colombia using custom layers and models in <a href="https://www.tensorflow.org/">TensorFlow 2</a>.</p>

<h2 id="install-qmc">Install <code class="language-plaintext highlighter-rouge">qmc</code></h2>

<p>Let¬¥s install the module <code class="language-plaintext highlighter-rouge">qmc</code> which contains</p>

<ol>
  <li><strong>Custom models</strong> inherited from the super class <code class="language-plaintext highlighter-rouge">tf.keras.Model</code>, in our case we will use:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">QMDensity()</code></li>
    </ul>
  </li>
  <li><strong>Custom layers</strong> inherited from the super class <code class="language-plaintext highlighter-rouge">tf.keras.layers.Layer</code>, we will take:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">QFeatureMapRFF()</code></li>
      <li><code class="language-plaintext highlighter-rouge">QMeasureDensity()</code></li>
    </ul>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">fagonzalezo</span><span class="o">/</span><span class="n">qmc</span><span class="p">.</span><span class="n">git</span>
</code></pre></div></div>

<p>For more information about algorithms found in <code class="language-plaintext highlighter-rouge">qmc</code> you can</p>

<h2 id="define-visualization-tools">Define visualization tools</h2>

<p>First we use the iPython functionality to show richer outputs using the magic command <code class="language-plaintext highlighter-rouge">%matplotlib inline</code> and force visualizations to be printed.</p>

<p>Then we import two important libraries:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">numpy</code> : Support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</li>
  <li><code class="language-plaintext highlighter-rouge">pylab</code> : Matplotlib is the toolkit, PyPlot is an interactive way to use Matplotlib and PyLab is the same thing as PyPlot but with some extra shortcuts. Using PyLab is discouraged now.</li>
</ul>

<p>Now let¬¥s create three functions for ploting purposes:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">plot_data(X,y)</code>: It has two args, one is the set of points in \(\mathbb{R}^2\) and the second one is a vector which contains the class of every point given. It returns a 2D scatter plot with points colored in a rainbow fashion according to their class.</li>
  <li><code class="language-plaintext highlighter-rouge">plot_decision_region(X,pred_fun)</code>: It has two args, one is the vector which will define the 2D square plot region(min_x,max_x,vector of points to evaluate the prediction function,‚Ä¶) and the other argument is the prediction function which we are interested to plot. The function returns a 2D contour plot and the points colored with the real class which they belong to.</li>
  <li><code class="language-plaintext highlighter-rouge">gen_pred_fun</code>: It has one arg which is a classifier which has a <code class="language-plaintext highlighter-rouge">.predict()</code> method and from this this functions extracts the predicted values.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pylab</span> <span class="k">as</span> <span class="n">pl</span>

<span class="c1"># Function to visualize a 2D dataset
</span><span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Subsets y with unique elements in a new array
</span>    <span class="n">y_unique</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># Generates a number of colors in rainbow according to the y_unique.size
</span>    <span class="n">colors</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">y_unique</span><span class="p">.</span><span class="n">size</span><span class="p">))</span>    
    <span class="k">for</span> <span class="n">this_y</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y_unique</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
        <span class="n">this_X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">this_y</span><span class="p">]</span>
        <span class="n">pl</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">this_X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>  <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span>
                    <span class="n">label</span><span class="o">=</span><span class="s">"Class %s"</span> <span class="o">%</span> <span class="n">this_y</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"best"</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Data"</span><span class="p">)</span>
    
<span class="c1"># Function to visualize the decission surface of a classifier
</span><span class="k">def</span> <span class="nf">plot_decision_region</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pred_fun</span><span class="p">):</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">max_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">min_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">max_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">min_x</span> <span class="o">=</span> <span class="n">min_x</span> <span class="o">-</span> <span class="p">(</span><span class="n">max_x</span> <span class="o">-</span> <span class="n">min_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
    <span class="n">max_x</span> <span class="o">=</span> <span class="n">max_x</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_x</span> <span class="o">-</span> <span class="n">min_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
    <span class="n">min_y</span> <span class="o">=</span> <span class="n">min_y</span> <span class="o">-</span> <span class="p">(</span><span class="n">max_y</span> <span class="o">-</span> <span class="n">min_y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
    <span class="n">max_y</span> <span class="o">=</span> <span class="n">max_y</span> <span class="o">+</span> <span class="p">(</span><span class="n">max_y</span> <span class="o">-</span> <span class="n">min_y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span>
    <span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_x</span><span class="p">,</span> <span class="n">max_x</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_y</span><span class="p">,</span> <span class="n">max_y</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">y_vals</span><span class="p">)</span>
    <span class="n">grid_r</span><span class="p">,</span> <span class="n">grid_c</span> <span class="o">=</span> <span class="n">XX</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">vals</span> <span class="o">=</span> <span class="p">[[</span><span class="n">XX</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">YY</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_r</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid_c</span><span class="p">)]</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">pred_fun</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>
    <span class="n">ZZ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="p">(</span><span class="n">grid_r</span><span class="p">,</span> <span class="n">grid_c</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">preds</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">ZZ</span><span class="p">))</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">CS</span> <span class="o">=</span> <span class="n">pl</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">levels</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="o">*</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)])</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">CS</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
    <span class="n">pl</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"y"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gen_pred_fun</span><span class="p">(</span><span class="n">clf</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">pred_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">clf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">pred_fun</span>
</code></pre></div></div>

<h2 id="import-qmctflayers-and-qmctfmodels">Import <code class="language-plaintext highlighter-rouge">qmc.tf.layers</code> and <code class="language-plaintext highlighter-rouge">qmc.tf.models</code></h2>

<p>Also we import</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">accuracy_score</code>: For calculating how good a classifier is. Thus counting the good predictions and dividing by the total of elements in the test set.</li>
  <li><code class="language-plaintext highlighter-rouge">make_blobs, make_moons, make_circles</code>: Datasets for doing classification.</li>
  <li><code class="language-plaintext highlighter-rouge">MinMaxScaler, OneHotEncoder</code>: MinMaxScaler scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one. OneHotEncoder maps a categorical value into binary variables corresponding to the number of classes in the categorical variable.</li>
  <li><code class="language-plaintext highlighter-rouge">train_test_split</code>: Spliting the data.</li>
  <li><code class="language-plaintext highlighter-rouge">tf</code>: Tensorflow.</li>
  <li><code class="language-plaintext highlighter-rouge">layers</code>: Custom layers in qmc.</li>
  <li><code class="language-plaintext highlighter-rouge">models</code>: Custom models in qmc.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span><span class="p">,</span> <span class="n">make_moons</span><span class="p">,</span> <span class="n">make_circles</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">qmc.tf.layers</span> <span class="k">as</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">qmc.tf.models</span> <span class="k">as</span> <span class="n">models</span>
</code></pre></div></div>

<h2 id="kernel-density-estimation--classical-approach">Kernel density estimation : Classical approach</h2>

<p>We first import two distributions <code class="language-plaintext highlighter-rouge">norm</code> and <code class="language-plaintext highlighter-rouge">bernoulli</code>. Then <code class="language-plaintext highlighter-rouge">gaussian_kde</code> for kernel density estimation using gaussian kernels.</p>

<p>Then we define the class <code class="language-plaintext highlighter-rouge">Mixture</code> inherited from <code class="language-plaintext highlighter-rouge">object</code> for defining a two component gaussian mixture</p>

\[f (x) = \alpha\left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)\right) + \left(1-\alpha \right) \left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_2)^2}{2\sigma_2^2}\right)\right).\]

<p>In the constructor <code class="language-plaintext highlighter-rouge">__init__</code> we define five parameters: four of them are related to the two univariate normals (<code class="language-plaintext highlighter-rouge">loc1</code>,<code class="language-plaintext highlighter-rouge">loc2</code>,<code class="language-plaintext highlighter-rouge">scale1</code> and <code class="language-plaintext highlighter-rouge">scale2</code>) and the other one <code class="language-plaintext highlighter-rouge">alpha</code> for the weights of the mixture. With the location and scale parameters we define two normals and assign them to <code class="language-plaintext highlighter-rouge">var1</code> and <code class="language-plaintext highlighter-rouge">var2</code>.</p>

<p>And two methods</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">pdf</code> which returns the mixture of two gaussians.</li>
  <li><code class="language-plaintext highlighter-rouge">rvs</code> which returns a sample of the mixture. Remember that first choose a mixture component by drawing \(j\) from the categorical distribution with probabilities \([\pi_1,\dots,\pi_d]\). This can be done using a random number generator for the categorical distribution. Note. In our case we only have two components so the ‚Äúcategorical distribution‚Äù reduces to a bernoulli distribution.</li>
</ul>

<p>Now we create an object <code class="language-plaintext highlighter-rouge">mixt</code> from <code class="language-plaintext highlighter-rouge">Mixture</code> class with some given parameters. That give us a ‚Äúbimodal‚Äù shape curve for the density.</p>

<p>Then we extract a sample of <code class="language-plaintext highlighter-rouge">size = 100</code> from mixt using the <code class="language-plaintext highlighter-rouge">rvs</code> method. Save it to <code class="language-plaintext highlighter-rouge">sample</code>.</p>

<p>In <code class="language-plaintext highlighter-rouge">kernel</code> we save the \(\hat f_{kde}\) estimated using <strong>kernel density estimation</strong> in a classical non parametric fashion applied to <code class="language-plaintext highlighter-rouge">sample</code> vector. Thus using <code class="language-plaintext highlighter-rouge">gaussian_kde</code>. Notice <code class="language-plaintext highlighter-rouge">gaussian_kde</code> includes automatic bandwidth determination.</p>

<p>Finally we plot <strong>mixt</strong> \(f\) and <strong>kernel</strong> \(\hat f_{kde}\) using <code class="language-plaintext highlighter-rouge">pl.plot</code> for a windows plot vector <code class="language-plaintext highlighter-rouge">x</code>.</p>

<p>As we want to compare our <em>quantum</em> way of estimating the density, we have to start out by defining a theoretical baseline model we want to approximate, in our case is <code class="language-plaintext highlighter-rouge">mixt</code>. Finally, the estimation method we would like to compare our model with is non pararmetric kernel density estimation, so we save this estimation in <code class="language-plaintext highlighter-rouge">kernel</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">bernoulli</span><span class="p">,</span> <span class="n">gaussian_kde</span>

<span class="k">class</span> <span class="nc">Mixture</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loc1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">loc2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">var1</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">var2</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        
    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">var1</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> 
                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">var2</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">var1</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">var2</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="p">)],</span>  <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">vals</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">vals</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)])</span>

<span class="n">n_var</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mixt</span> <span class="o">=</span> <span class="n">Mixture</span><span class="p">(</span><span class="n">loc1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">loc2</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale2</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">mixt</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> 
<span class="n">kernel</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">,</span><span class="mi">99</span><span class="p">)</span>
<span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mixt</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'r-'</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'norm pdf'</span><span class="p">)</span>
<span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'b-'</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'kde pdf'</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="sample-from-the-guassian-mixture-model">Sample from the Guassian Mixture Model</h3>

<p>First we stack the two random samples from the two gaussian densities in <code class="language-plaintext highlighter-rouge">valstest</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">valstest</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">),</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span><span class="p">).</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">valstest</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[-0.14509451,  9.85286915],
       [ 0.34545315, 11.22459855],
       [-0.38473518,  9.34825703],
       [-0.88421728,  8.82330827]])
</code></pre></div></div>

<p>Now we generate the indexes that will help us to choose the component according to <code class="language-plaintext highlighter-rouge">alpha</code> and the <code class="language-plaintext highlighter-rouge">size</code> of the sample we want (comes from the last step). Notice that we have two components in the mixture, so our vector will contain either <code class="language-plaintext highlighter-rouge">0</code> or <code class="language-plaintext highlighter-rouge">1</code>. We recall that here we can use a bernoulli since it has two possible outcomes, but if we¬¥d had N components to sample from we would use a categorical distribution in this step.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">idxtest</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">idxtest</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0, 1, 1, 1])
</code></pre></div></div>

<p>Now we choose our desired sample from <code class="language-plaintext highlighter-rouge">valstest</code> according to the index in <code class="language-plaintext highlighter-rouge">idxtest</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">valstest</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">idxtest</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([-0.14509451, 11.22459855,  9.34825703,  8.82330827])
</code></pre></div></div>

<h2 id="kernel-density-estimation-using-density-matrices">Kernel density estimation using Density Matrices</h2>

<hr />

<h3 id="some-context-on-the-layers-to-be-used-in-qmdensity">Some context on the layers to be used in <code class="language-plaintext highlighter-rouge">QMDensity()</code></h3>

<p>To run the <code class="language-plaintext highlighter-rouge">dmkde</code> method for estimating the density \(f\) we do some things first.</p>

<p>Let¬¥s remember that we generated a sample from the gaussian mixture model and save it to <code class="language-plaintext highlighter-rouge">sample</code>. <code class="language-plaintext highlighter-rouge">sample</code> shape is <code class="language-plaintext highlighter-rouge">(3,)</code> so we first reshape it to <code class="language-plaintext highlighter-rouge">(3,1)</code> in order to have a sample with the correct dimensions into our model.</p>

<p>Define a dimension variable <code class="language-plaintext highlighter-rouge">dim = 300</code> which will be the number of random Fourier features.</p>

<p>Now we create an object feature map for values in the <code class="language-plaintext highlighter-rouge">x</code> dimension into the variable <code class="language-plaintext highlighter-rouge">fm_x</code>. Let¬¥s remember that the class <code class="language-plaintext highlighter-rouge">QFeatureMapRFF</code> is inherited from <code class="language-plaintext highlighter-rouge">tf.keras.Layer</code> whose initialization parameters are</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    input_dim: dimension of the input
    dim: int. Number of dimensions to represent a sample.
    gamma: float. Gamma parameter of the RBF kernel to be approximated.
    random_state: random number generator seed.
</code></pre></div></div>

<p>And let‚Äôs recall that in order to generate random fourier features we must</p>

<ol>
  <li>Choose the number of Random Fourier Features \(R\).</li>
  <li>Draw $R$ samples \(\{\mathbf{w}_1,\dots,\mathbf{w}_R\}\) from a <strong>gaussian distribution</strong> \(\mathcal{N}(\mathbf{0},\mathbf{I_d})\) since Bochner theorem asserts that \(K(\Delta) = \int exp(iw\Delta)p(w)dw\) and we know that under the Fourier transform, the Gaussian function is mapped to another Gaussian function with a different width.</li>
  <li>Draw \(R\) samples \(\{b_1,\dots,b_R\}\) from an uniform distribution in \((0,2\pi)\).</li>
  <li>Our desired random map will be \(\phi_{rff}: \mathbb{R}^d \rightarrow \mathbb{R}^R\) such that
\(\phi_{rff}(\mathbf{x}) = \begin{pmatrix} \frac{1}{\sqrt{R}} \sqrt{2}cos(\mathbf{w_1^Tx }+b) \\ \vdots \\ \frac{1}{\sqrt{R}} \sqrt{2}cos(\mathbf{w_R^Tx }+b)\end{pmatrix}\)</li>
</ol>

<p>Notice that in order to build such map we need to do the following matching between math elements and variables in the constructor</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">input_dim</code> : \(d\)</li>
  <li><code class="language-plaintext highlighter-rouge">dim</code> : \(R\)</li>
  <li><code class="language-plaintext highlighter-rouge">gamma</code> : Bandwidth parameter of the internal gaussian distribution.</li>
</ul>

<p>Notice that at the end with end up with \(\psi =\) <code class="language-plaintext highlighter-rouge">norm (tf.cos(w^tx +b)* tf.sqrt(2. / self.dim) ) / tf.expand_dims(norm(vector), axis=-1)</code>.</p>

<p>Understand what <code class="language-plaintext highlighter-rouge">RBFSampler</code> does ? estimates the internal kernel ? Or the outside kernel ? Why to define such a big layer when you have a direct method that approximates the kernel using <code class="language-plaintext highlighter-rouge">RBFSampler</code>.</p>

<h3 id="how-does-qfeaturemaprff-class-work-">How does <code class="language-plaintext highlighter-rouge">QFeatureMapRFF</code> class work ?</h3>

<p>Behind the functioning of this layer is the RBF sampler which approximates a Radial Basis Function (the gaussian belongs to this class). From instancing and <code class="language-plaintext highlighter-rouge">RBFSampler</code> we extract the weights $\mathbf{w_i}$‚Äôs and the offsets $b_i$‚Äôs and build the Quantum Feature Map.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># d = 1  
</span><span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">RBFSampler</span>

<span class="n">rbf_sampler</span> <span class="o">=</span> <span class="n">RBFSampler</span><span class="p">(</span>
            <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">n_components</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">rbf_sampler</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">rbf_sampler</span><span class="p">.</span><span class="n">random_weights_</span><span class="p">.</span><span class="n">shape</span>
<span class="n">rbf_sampler</span><span class="p">.</span><span class="n">random_offset_</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(300,)
</code></pre></div></div>

<p>What is doing the layer when passing a vector of 300</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fm_x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">QFeatureMapRFF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">fm_x</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)).</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TensorShape([3, 300])
</code></pre></div></div>

<h3 id="how-does-qmeasuredensity-layer-class-work-">How does <code class="language-plaintext highlighter-rouge">QMeasureDensity</code> layer class work ?</h3>

<p>In this class we compute the value of the density by doing:</p>

\[\rho = \frac{1}{N} \sum_i^Nz_iz_i^*\]

<p>where \(N\) is the number of samples.</p>

<h3 id="training-a-qmdensity-model">Training a <code class="language-plaintext highlighter-rouge">QMDensity</code> model</h3>

<p>Here we just use the <code class="language-plaintext highlighter-rouge">QMDensity</code> model which has the following structure:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class QMDensity(tf.keras.Model):
    """
    A Quantum Measurement Density Estimation model.
    Arguments:
        fm_x: Quantum feature map layer for inputs
        dim_x: dimension of the input quantum feature map
    """
    def __init__(self, fm_x, dim_x):
        super(QMDensity, self).__init__()
        self.fm_x = fm_x
        self.dim_x = dim_x
        self.qmd = layers.QMeasureDensity(dim_x)
        self.cp = layers.CrossProduct()
        self.num_samples = tf.Variable(
            initial_value=0.,
            trainable=False     
            )

    def call(self, inputs):
        psi_x = self.fm_x(inputs)
        probs = self.qmd(psi_x)
        return probs

    @tf.function
    def call_train(self, x):
        if not self.qmd.built:
            self.call(x)
        psi = self.fm_x(x)
        rho = self.cp([psi, tf.math.conj(psi)])
        num_samples = tf.cast(tf.shape(x)[0], rho.dtype)
        rho = tf.reduce_sum(rho, axis=0)
        self.num_samples.assign_add(num_samples)
        return rho

    def train_step(self, data):
        x = data
        rho = self.call_train(x)
        self.qmd.weights[0].assign_add(rho)
        return {}

    def fit(self, *args, **kwargs):
        result = super(QMDensity, self).fit(*args, **kwargs)
        self.qmd.weights[0].assign(self.qmd.weights[0] / self.num_samples)
        return result

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

</code></pre></div></div>

<p>In general it has two layers and they are defined in the <code class="language-plaintext highlighter-rouge">__init__</code>:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">fm_x</code> = Quantum feature map which is a layer of <code class="language-plaintext highlighter-rouge">QFeatureMapFRR</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">qmd</code> = Which is a layer that actually do the measurement which is a layer of type <code class="language-plaintext highlighter-rouge">QMeasureDensity(dim_x)</code>.</li>
</ul>

<p>These two layers are concatenated and we can see this model‚Äôs forward pass inside the <code class="language-plaintext highlighter-rouge">call</code> method.</p>

<p>Then a customization is made at the level of training the model using auxiliary methods such as <code class="language-plaintext highlighter-rouge">call_train</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">sample</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">fm_x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">QFeatureMapRFF</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">17</span><span class="p">)</span>
<span class="n">qmd</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">QMDensity</span><span class="p">(</span><span class="n">fm_x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="n">qmd</span><span class="p">.</span><span class="nb">compile</span><span class="p">()</span>
<span class="n">qmd</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">qmd</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mixt</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'r-'</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'norm pdf'</span><span class="p">)</span>
<span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">'b-'</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'kde pdf'</span><span class="p">)</span>
<span class="n">pl</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="s">'g-'</span><span class="p">,</span>  <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'qmkde pdf'</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="/img/posts/Part2_DMKDE/DMKDE_40_2.png" alt="png" /></p>

<h2 id="further-reading">Further reading</h2>

<ul>
  <li>More information on customization when using <code class="language-plaintext highlighter-rouge">tf</code> this can be found at <a href="https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers">here</a>.</li>
</ul>
:ET