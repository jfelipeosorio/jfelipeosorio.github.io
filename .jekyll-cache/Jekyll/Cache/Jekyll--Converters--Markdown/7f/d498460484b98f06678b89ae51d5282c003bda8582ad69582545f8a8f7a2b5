I"À<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p>Density estimation methods can be used to solve a variety of statistical and machine learning challenges. They can be used to tackle a variety of problems, including anomaly detection, generative models, semi-supervised learning, compression, and text-to-speech.  A popular technique to find density estimates for new samples in a non parametric set up is Kernel Density Estimation (KDE), a method which suffers from costly evaluations especially for large data sets and higher dimensions. In this post we will discuss on the math behind an efficient way to calculate density estimates using density matrices (a concept from quantum mechanics).</p>

<h2 id="introduction">Introduction</h2>
<p>One of the principal methodologies for analyzing data is assuming its random nature and modelling its probability behaviour. In many applications we have a finite
set of data and we would like to know what probability distribution generated the
data. From statistical inference this problem has played a central role in research
and has inspired many methods which rely on the use of the density function such
as non parametric regression when non linear patterns are observed. Also in machine learning many approaches to anomaly detection make use of the probability
density function.</p>

<p>The parametric approach to density estimation given some data \(\mathbf{x}_1,\dots,\mathbf{x}_N\) assumes that each \(\mathbf{x}_i\) is sampled independently from a random vector \(\mathbf{X}\sim f(\mathbf{x};\mathbf{\theta})\) and the theory is developed around building an estimator \(\hat{\mathbf{\theta}}\) with good statistical properties such us unbiasdness, consistency, efficiency and sufficiency. The probability density of a new sample \(\mathbf{x}\) is given by:
\[
\hat{f}(\mathbf{x}) = f(\mathbf{x};\hat{\mathbf{\theta}})
\]</p>

<p>Another approach to get density estimations in new samples in a non-parametric fashion is KDE and it can be understood as a weighted sum of density contributions that are centered at each data point. Formally, given a univariate random sample \(X_1,...,X_N\) from an unknown distribution with density \(f\), the KDE estimator of the density at a query point \(x\in\mathbb{R}\) is given by</p>

<p>\[
    \hat{f}(x) = \frac{1}{Nh}\sum_{i=1}^N K\left(\frac{x-X_i}{h}\right).
\]</p>

<p>where \(h\) is called the <em>bandwidth</em> and \(K:\mathbb{R}\rightarrow \mathbb{R}_{\geq 0}\) is a positive definite function and its called the <em>kernel function</em>.</p>

<p>Notice that a naive direct evaluation of KDE at \(m\) query points for \(N\) samples requires \(O(mN)\) kernel evaluations and \(O(mN)\) additions and multiplications. Also if we restrict to \(N\) query points then we get a computational complexity of \(O(N^2)\), making it a very expensive trade-off, especially for large data sets and higher dimensions (see more <a href="https://link.springer.com/book/10.1007/978-3-319-71688-6">here</a>).</p>

<p>According to <a href="https://proceedings.mlr.press/v97/siminelakis19a.html">Siminelakis</a>, one technique to resolve the problem of scalability of the na√Øve evaluation of KDE in the literature is on discovering fast approximate evaluation of the kernel, and there are two primary lines of effort: space partitioning methods and Monte Carlo random sampling.</p>

<p>The formalism of density operators and density matrices was developed by Von Neumann as a foundation of quantum statistical mechanics.  From the pointof view of machine learning,  density matrices have an interesting feature: the fact that they combine linear algebra and probability, two of the pillars of machine learning, in a very particular but powerful way.</p>

<p>The central idea of this post is to use density matrices to represent probability distributions tackling the important drawback of scalability and create a competitive strategy to compute densities on new samples.</p>

<h2 id="math-background">Math background</h2>
<h3 id="kde">KDE</h3>
<p>The multivariate kernel density estimator at a query point \(\mathbf{x}\in\mathbb{R}^d\) for a given random sample \(\mathbf{X}_1,\dots,\mathbf{X}_N\) drawn from an unknown density \(f\) is given by</p>

\[\hat{f}_\gamma(\mathbf{x})=\frac{1}{N(\pi / \gamma)^{\frac{d}{2}}} \sum_{i=1}^{N} \exp \left({-\gamma\left\|\mathbf{x}-\mathbf{X}_{i}\right\|^{2}}\right)\]

<p>where we define $\gamma = \frac{1}{2\sigma}$ and assume a Gaussian kernel.</p>

<h2 id="text">Text</h2>
<p>We are gonna have so much fun.</p>

<h2 id="math">Math</h2>
<p>Centered math
\[ x = {-b \pm \sqrt{b^2-4ac} \over 2a} \]
Inline math \(x^2\).</p>

<h2 id="images">Images</h2>
<p><img src="/img/posts/First-post/alldensities_pot3.jpeg" alt="imagen" /></p>

<h1 id="code">Code</h1>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = input()

</code></pre></div></div>

:ET