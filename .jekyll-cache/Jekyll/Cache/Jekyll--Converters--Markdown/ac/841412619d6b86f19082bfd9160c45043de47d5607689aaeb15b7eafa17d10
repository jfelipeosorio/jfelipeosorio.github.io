I"“ <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p>Density estimation methods can be used to solve a variety of statistical and machine learning challenges. They can be used to tackle a variety of problems, including anomaly detection, generative models, semi-supervised learning, compression, and text-to-speech.  A popular technique to find density estimates for new samples in a non parametric set up is Kernel Density Estimation (KDE), a method which suffers from costly evaluations especially for large data sets and higher dimensions. In this post we will discuss on the math behind an efficient way to calculate density estimates using density matrices (a concept from quantum mechanics).</p>

<h2 id="introduction">Introduction</h2>
<p>One of the principal methodologies for analyzing data is assuming its random nature and modelling its probability behaviour. In many applications we have a finite
set of data and we would like to know what probability distribution generated the
data. From statistical inference this problem has played a central role in research
and has inspired many methods which rely on the use of the density function such
as non parametric regression when non linear patterns are observed. Also in machine learning many approaches to anomaly detection make use of the probability
density function.</p>

<p>The parametric approach to density estimation given some data \(\mathbf{x}_1,\dots,\mathbf{x}_N\) assumes that each \(\mathbf{x}_i\) is sampled independently from a random vector \(\mathbf{X}\sim f(\mathbf{x};\mathbf{\theta})\) and the theory is developed around building an estimator \(\hat{\mathbf{\theta}}\) with good statistical properties such us unbiasdness, consistency, efficiency and sufficiency. The probability density of a new sample \(\mathbf{x}\) is given by:
\[
\hat{f}(\mathbf{x}) = f(\mathbf{x};\hat{\mathbf{\theta}})
\]</p>

<p>Another approach to get density estimations in new samples in a non-parametric fashion is KDE and it can be understood as a weighted sum of density contributions that are centered at each data point. Formally, given a univariate random sample \(X_1,...,X_N\) from an unknown distribution with density \(f\), the KDE estimator of the density at a query point \(x\in\mathbb{R}\) is given by</p>

<p>\[
    \hat{f}(x) = \frac{1}{Nh}\sum_{i=1}^N K\left(\frac{x-X_i}{h}\right).
\]</p>

<p>where \(h\) is called the <em>bandwidth</em> and \(K:\mathbb{R}\rightarrow \mathbb{R}_{\geq 0}\) is a positive definite function and its called the <em>kernel function</em>.</p>

<p>Notice that a naive direct evaluation of KDE at \(m\) query points for \(N\) samples requires \(O(mN)\) kernel evaluations and \(O(mN)\) additions and multiplications. Also if we restrict to \(N\) query points then we get a computational complexity of \(O(N^2)\), making it a very expensive trade-off, especially for large data sets and higher dimensions (see more <a href="https://link.springer.com/book/10.1007/978-3-319-71688-6">here</a>).</p>

<p>According to <a href="https://proceedings.mlr.press/v97/siminelakis19a.html">Siminelakis</a>, one technique to resolve the problem of scalability of the naÃ¯ve evaluation of KDE in the literature is on discovering fast approximate evaluation of the kernel, and there are two primary lines of effort: space partitioning methods and Monte Carlo random sampling.</p>

<p>The formalism of density operators and density matrices was developed by Von Neumann as a foundation of quantum statistical mechanics.  From the pointof view of machine learning,  density matrices have an interesting feature: the fact that they combine linear algebra and probability, two of the pillars of machine learning, in a very particular but powerful way.</p>

<p>The central idea of this post is to use density matrices to represent probability distributions tackling the important drawback of scalability and create a competitive strategy to compute densities on new samples.</p>

<p>Now letâ€™s define and discuss important mathematical background for the method.</p>

<h2 id="multivariate-version-of-kernel-density-estimation">Multivariate version of Kernel Density Estimation</h2>

<p>The multivariate kernel density estimator at a query point \(\mathbf{x}\in\mathbb{R}^d\) for a given random sample \(\mathbf{X}_1,\dots,\mathbf{X}_N\) drawn from an unknown density \(f\) is given by</p>

\[\hat{f}_\gamma(\mathbf{x})=\frac{1}{N(\pi / \gamma)^{\frac{d}{2}}} \sum_{i=1}^{N} \exp \left({-\gamma\left\|\mathbf{x}-\mathbf{X}_{i}\right\|^{2}}\right)\]

<p>where we define \(\gamma = \frac{1}{2\sigma}\) and assume a Gaussian kernel.</p>

<h2 id="random-fourier-features">Random Fourier Features</h2>

<p>The use of Random Fourier Features (RFF) was explicitly introduced by <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Rahimi and Recht</a> in 2007. In this methodology authors approximate a shift invariant kernel by an inner product in an explicit Hilbert space. Formally, given a shift-invariant kernel \(k: \mathbb{R}^d\times\mathbb{R}^d \rightarrow \mathbb{R}\) they build a map \(\phi_{\text{rff}} : \mathbb{R}^d \rightarrow \mathbb{R}^D\) such that</p>

<p>\[
k(\mathbf{x},\mathbf{y}) \approx \phi_{\text{rff}}(\mathbf{x})^T \phi_{\text{rff}}(\mathbf{y})
\]</p>

<p>for all \(\{\mathbf{x},\mathbf{y}\}\subseteq \mathbb{R}^d\). The main theoretical result that supports the last equation comes from an instance of <a href="https://en.wikipedia.org/wiki/Bochner%27s_theorem">Bochnerâ€™s theorem</a> which shows that a shift invariant positive-definite kernel \(k\) is the Fourier transform of a probability measure \(p\). Specifically, to approximate the integral in the Fourier transform we sample \(\mathbf{\omega}_1,\dots,\mathbf{\omega}_D\) from \(p\) and \(b_1,\dots,b_D\) from a uniform distribution in the interval \((0,2\pi)\) to define the map \(\phi_{\text{rff}}:\mathbb{R}^d\rightarrow\mathbb{R}^D\) as</p>

<p>\[
    \phi_{\text{rff}}(\mathbf{x})=\left(
\frac{1}{\sqrt{D}} \sqrt{2} \cos \left(\boldsymbol{\omega}_1^{\top} \mathbf{x}+b_1\right),
\dots,
\frac{1}{\sqrt{D}} \sqrt{2} \cos \left(\boldsymbol{\omega}_D^{\top} \mathbf{x}+b_D\right)
\right)^T
\]</p>

<p>Rahimi and Recht also showed that the expected value of \(\phi_{\text{rff}}(\mathbf{x})\phi_{\text{rff}}^T(\mathbf{y})\) uniformly converges to \(k(\mathbf{x}, \mathbf{y})\). In general, we can reduce the complexity of kernel methods using this methodology because kernel evaluations are faster.</p>

<h2 id="density-matrices">Density Matrices</h2>

<p>The state of a quantum system is represented by a vector \(\varphi\in\mathcal{H}\) where \(\mathcal{H}\) is the Hilbert space of the possible states
of the system and typically \(H=\mathbb{C}^d\).</p>

<p>Letâ€™s considerer the example of the spin of an electron. The state vector is given by</p>

<p>\[
    \varphi = (\alpha,\beta)^T, \quad |\alpha|^2+|\beta|^2=1
\]</p>

<p>In general, the quantum state vector is a combination of basis states and this is called <em>superposition</em>. In the example the basis states are</p>

<p>\[
    \uparrow := (1,0)^T, \quad \downarrow :=(0,1)^T
\]</p>

<p>with \(\lVert \alpha \rVert^2\) and \(\lVert \beta  \rVert^2\) being the probabilities of obtaining the corresponding basis state. A <em>density matrix</em> is a representation of the state of a quantum system that can represent quantum uncertainty (superposition) and classical uncertainty. We can express the state of the quantum system in one of two ways, depending on whether or not there is classical uncertainty.</p>

<table>
  <thead>
    <tr>
      <th>Pure</th>
      <th>Mixed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(x^2\)</td>
      <td>Title</td>
    </tr>
    <tr>
      <td>Paragraph</td>
      <td>Text</td>
    </tr>
  </tbody>
</table>

<h2 id="math">Math</h2>
<p>Centered math
\[ x = {-b \pm \sqrt{b^2-4ac} \over 2a} \]
Inline math \(x^2\).</p>

<h2 id="images">Images</h2>
<p><img src="/img/posts/First-post/alldensities_pot3.jpeg" alt="imagen" /></p>

<h1 id="code">Code</h1>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a = input()

</code></pre></div></div>

:ET