I"◊<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<p>A powerful set of methods in machine learning arise from the use of kernels. One of the main results from these include the so-called <em>representer theorem</em> which allows us make tracktacle the inference of a desired approximation function. In this post we will discuss the math behind the proofs of the representer theorem and its generalizations.</p>

<h2 id="classical-representer-theorem">Classical representer theorem</h2>

<p>Let \(K\) be a reproducing kernel, and denote by \(\mathcal{H}_K\) its <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">RKHS</a>. Suppose \(\mathcal{X}\) be a non-empty set and that we are given:</p>

<ul>
  <li>
    <p>Data set: \(\left\{(x_i,y_i)\right\}_{i=1}^{N}\subset \mathcal{X} \times \mathbb{R}\) where \(d\geq 1\) and \(N &gt; 1\).</p>
  </li>
  <li>
    <p>Cost function: \(L :(\mathbb{R}^d \times \mathbb{R} \times \mathbb{R})^N \to \mathbb{R} \cup \{\infty\}\).</p>
  </li>
  <li>
    <p>Strictly monotonically increasing function: \(g : [0,\infty) \to \mathbb{R}\).</p>
  </li>
</ul>

<p>then any element from the set</p>

\[\underset{f\in\mathcal{H}_K}{\text{argmin }} L\left(\left(x_i,y_i,f(x_i)\right)_{i=1}^N\right) + g(\|f\|_{\mathcal{H}_K})\]

<p>is of the form</p>

\[f(\cdot) = \sum_{i=1}^N \beta_i K(\cdot,x_i)\]

<p>where \(\beta \in \mathbb{R}^N\).</p>

<p><em>Proof:</em> Since the set \(H_0 = \text{span}\{K(\cdot,x_1),\dots,K(\cdot,x_N)\}\) is isomorphic to \(\mathbb{R}^N\) (which is complete), then it is a closed subspace of \(\mathcal{H}_K\) so</p>

\[\mathcal{H}_K = H_0 \oplus H_0^\perp.\]

<p>Thus, if \(f\in \mathcal{H}_K\) then there exists \(\beta \in \mathbb{R}^N\) and \(w \in H_0^\perp\) such that</p>

\[f(\cdot) = \sum_{i=1}^N \beta_i K(\cdot,x_i) + w(\cdot).\]

<p>Notice that for any \(j\in\{1,\dots,N\}\) we have</p>

\[f(x_j) = \sum_{i=1}^N \beta_i K(x_j,x_i) + w(x_j) = \sum_{i=1}^N \beta_i K(x_j,x_i)  + \langle w,K(x_j,\cdot) \rangle = \sum_{i=1}^N \beta_i K(x_j,x_i)\]

<p>which means that when we evaluate \(f\) at any of the training points we only care about the first part, i.e., \(f(x_j)\) does not depend on evaluating \(w\) at $x_j$, and we conclude that \(L\left(\left(x_i,y_i,f(x_i)\right)_{i=1}^N\right)\) does not depend on \(w(x_j)\). On the other hand, notice that</p>

\[\begin{align*}
g(\|f\|_{\mathcal{H}_K}) &amp;= g(\|\sum_{i=1}^N \beta_i K(\cdot,x_i) + w(\cdot)\|_{\mathcal{H}_K}) \\
&amp;= g((\|\sum_{i=1}^N \beta_i K(\cdot,x_i) + w(\cdot)\|_{\mathcal{H}_K}^2)^{1/2}) \\
&amp;= g((\|\sum_{i=1}^N \beta_i K(\cdot,x_i)\|_{\mathcal{H}_K}^2 + \|w(\cdot)\|_{\mathcal{H}_K}^2 + 2 \langle \sum_{i=1}^N \beta_i K(\cdot,x_i), w(\cdot) \rangle)^{1/2}) \\
&amp;= g((\|\sum_{i=1}^N \beta_i K(\cdot,x_i)\|_{\mathcal{H}_K}^2 + \|w(\cdot)\|_{\mathcal{H}_K}^2 )^{1/2}) \\
&amp;\geq g((\|\sum_{i=1}^N \beta_i K(\cdot,x_i)\|_{\mathcal{H}_K}^2 )^{1/2})\\
&amp;= g(\|\sum_{i=1}^N \beta_i K(\cdot,x_i)\|_{\mathcal{H}_K} ) 
\end{align*}\]

<p>where we have use the properties of \(g\) being monotonically strictly increasing and \(v \in H_0^\perp\).</p>

<p>Thus, we just saw that if we choose \(w \equiv 0\) then we always solve for the variational problem meaning that we must have that any minimizer is of the form</p>

\[f(\cdot) = \sum_{i=1}^N \beta_i K(\cdot,x_i)\]

<p>where \(\beta \in \mathbb{R}^N\).</p>

<h2 id="interpolation-version-discrete">Interpolation version (discrete)</h2>

<p>We usually assume that our pair of observations \(\{(x_i,y_i)\}_{i=1}^N\) have the following functional dependency</p>

\[y_i = f^\dagger(x_i) \quad \text{ for all } \quad i \in \{1,\dots,N\}\]

<p>so that we would like to find an approximation to \(f^\dagger\) by solving</p>

\[\underset{f\in\mathcal{H}_K}{\text{argmin }} \|f\|_{\mathcal{H}_K} \quad \text{ s.t. } \quad f(x_i) = f^\dagger(x_i)\]

<p>whose solution according to the classical result is of the form</p>

\[\hat f(x) = \sum_{i=1}^N K(x,x_i) \alpha_i\]

<p>for some \(\alpha \in \mathbb{R}^N\) that satisfies the linear system</p>

\[\sum_{i=1}^N K(x_i,x_j) \alpha_i = f(x_i) \quad \text{for all}\quad j \in \{1,\dots,N\}.\]

<h2 id="interpolation-version-continuous">Interpolation version (continuous)</h2>

<p>We have access to \(\{(x,y)\}_{x\in\Omega}\) have the following functional dependency</p>

\[y = f^\dagger(x) \quad \text{ for all } \quad x \in \Omega\]

<p>so that we would like to find an approximation to \(f^\dagger\) by solving</p>

\[\underset{f\in\mathcal{H}_K}{\text{argmin }} \|f\|_{\mathcal{H}_K} \quad \text{ s.t. } \quad f(x_i) = f^\dagger(x_i)\]

<p>whose solution according to the classical result is of the form</p>

\[\hat f(x) = \sum_{i=1}^N K(x,x_i) \alpha_i\]

<p>for some \(\alpha \in \mathbb{R}^N\) that satisfies the linear system</p>

\[\sum_{i=1}^N K(x_i,x_j) \alpha_i = f(x_i) \quad \text{for all}\quad j \in \{1,\dots,N\}.\]

<h2 id="references">References:</h2>

<ul>
  <li>For the classical result we refer to</li>
</ul>

<blockquote>
  <p>G. S. Kimeldorf and G. Wahba. Some results on Tchebycheffian spline functions.
J. Math. Anal. Applic., 33:82‚Äì95, 1971.</p>
</blockquote>

<blockquote>
  <p>Sch√∂lkopf, Bernhard; Herbrich, Ralf; Smola, Alex J. (2001). ‚ÄúA Generalized Representer Theorem‚Äù. In Helmbold, David; Williamson, Bob (eds.). Computational Learning Theory. Lecture Notes in Computer Science. Vol. 2111. Berlin, Heidelberg: Springer. pp. 416‚Äì426</p>
</blockquote>

:ET