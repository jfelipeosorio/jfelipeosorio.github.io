---
layout: post
title: "Part 2: Kernel Density Estimation using Density Matrices"
subtitle: "Implementation in TensorFlow2."
background: '' 
---
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 

In [Part 1]({% post_url 2021-12-22-Part1_DMKDE %}) we checked at the key mathematical and statistical concepts that could allow us to talk about Kernel Density Estimation using Density Matrices. In Part 2, we would like to describe how to use the algorithm already implemented in Python [here](https://github.com/fagonzalezo/qmc) by Professor [Fabio Gonzalez](https://dis.unal.edu.co/~fgonza/) and his research group [MindLab](http://www.ingenieria.unal.edu.co/mindlab/) at Universidad Nacional de Colombia using custom layers and models in [TensorFlow 2](https://www.tensorflow.org/). 


## Install `qmc` 

Let´s install the module `qmc` which contains

1. **Custom models** inherited from the super class `tf.keras.Model`, in our case we will use:
    - `QMDensity()`

2. **Custom layers** inherited from the super class `tf.keras.layers.Layer`, we will take:
    - `QFeatureMapRFF()`
    - `QMeasureDensity()`

```python
    !pip install git+https://github.com/fagonzalezo/qmc.git
```

For more information about algorithms found in `qmc` you can 





## Define visualization tools

First we use the iPython functionality to show richer outputs using the magic command `%matplotlib inline` and force visualizations to be printed.

Then we import two important libraries:
- `numpy` : Support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.
- `pylab` : Matplotlib is the toolkit, PyPlot is an interactive way to use Matplotlib and PyLab is the same thing as PyPlot but with some extra shortcuts. Using PyLab is discouraged now.

Now let´s create three functions for ploting purposes:

- `plot_data(X,y)`: It has two args, one is the set of points in $$\mathbb{R}^2$$ and the second one is a vector which contains the class of every point given. It returns a 2D scatter plot with points colored in a rainbow fashion according to their class.
- `plot_decision_region(X,pred_fun)`: It has two args, one is the vector which will define the 2D square plot region(min_x,max_x,vector of points to evaluate the prediction function,...) and the other argument is the prediction function which we are interested to plot. The function returns a 2D contour plot and the points colored with the real class which they belong to.
- `gen_pred_fun`: It has one arg which is a classifier which has a `.predict()` method and from this this functions extracts the predicted values.


```python
%matplotlib inline 
import numpy as np
import pylab as pl

# Function to visualize a 2D dataset
def plot_data(X, y):
    # Subsets y with unique elements in a new array
    y_unique = np.unique(y)
    # Generates a number of colors in rainbow according to the y_unique.size
    colors = pl.cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))    
    for this_y, color in zip(y_unique, colors):
        this_X = X[y == this_y]
        pl.scatter(this_X[:, 0], this_X[:, 1],  c=color,
                    alpha=0.5, edgecolor='k',
                    label="Class %s" % this_y)
    pl.legend(loc="best")
    pl.title("Data")
    
# Function to visualize the decission surface of a classifier
def plot_decision_region(X, pred_fun):
    min_x = np.min(X[:, 0])
    max_x = np.max(X[:, 0])
    min_y = np.min(X[:, 1])
    max_y = np.max(X[:, 1])
    min_x = min_x - (max_x - min_x) * 0.05
    max_x = max_x + (max_x - min_x) * 0.05
    min_y = min_y - (max_y - min_y) * 0.05
    max_y = max_y + (max_y - min_y) * 0.05
    x_vals = np.linspace(min_x, max_x, 50)
    y_vals = np.linspace(min_y, max_y, 50)
    XX, YY = np.meshgrid(x_vals, y_vals)
    grid_r, grid_c = XX.shape
    vals = [[XX[i, j], YY[i, j]] for i in range(grid_r) for j in range(grid_c)]
    preds = pred_fun(np.array(vals))
    ZZ = np.reshape(preds, (grid_r, grid_c))
    print(np.min(preds), np.min(ZZ))
    pl.contourf(XX, YY, ZZ, 100, cmap = pl.cm.coolwarm, vmin= 0, vmax=1)
    pl.colorbar()
    CS = pl.contour(XX, YY, ZZ, 100, levels = [0.1*i for i in range(1,10)])
    pl.clabel(CS, inline=1, fontsize=10)
    pl.xlabel("x")
    pl.ylabel("y")

def gen_pred_fun(clf):
    def pred_fun(X):
        return clf.predict(X)[:, 1]
    return pred_fun
```

## Import `qmc.tf.layers` and `qmc.tf.models`

Also we import
- `accuracy_score`: For calculating how good a classifier is. Thus counting the good predictions and dividing by the total of elements in the test set.
- `make_blobs, make_moons, make_circles`: Datasets for doing classification.
- `MinMaxScaler, OneHotEncoder`: MinMaxScaler scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one. OneHotEncoder maps a categorical value into binary variables corresponding to the number of classes in the categorical variable.
- `train_test_split`: Spliting the data.
- `tf`: Tensorflow.
- `layers`: Custom layers in qmc.
- `models`: Custom models in qmc.


```python
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_blobs, make_moons, make_circles
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
import qmc.tf.layers as layers
import qmc.tf.models as models
```

## Kernel density estimation : Classical approach

We first import two distributions `norm` and `bernoulli`. Then `gaussian_kde` for kernel density estimation using gaussian kernels.

Then we define the class `Mixture` inherited from `object` for defining a two component gaussian mixture

$$
f (x) = \alpha\left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_1)^2}{2\sigma_1^2}\right)\right) + \left(1-\alpha \right) \left(\frac{1}{\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_2)^2}{2\sigma_2^2}\right)\right).
$$

In the constructor `__init__` we define five parameters: four of them are related to the two univariate normals (`loc1`,`loc2`,`scale1` and `scale2`) and the other one `alpha` for the weights of the mixture. With the location and scale parameters we define two normals and assign them to `var1` and `var2`.

And two methods
- `pdf` which returns the mixture of two gaussians.
- `rvs` which returns a sample of the mixture. Remember that first choose a mixture component by drawing $$j$$ from the categorical distribution with probabilities $$[\pi_1,\dots,\pi_d]$$. This can be done using a random number generator for the categorical distribution. Note. In our case we only have two components so the "categorical distribution" reduces to a bernoulli distribution.

Now we create an object `mixt` from `Mixture` class with some given parameters. That give us a "bimodal" shape curve for the density.

Then we extract a sample of `size = 100` from mixt using the `rvs` method. Save it to `sample`.

In `kernel` we save the $$\hat f_{kde}$$ estimated using **kernel density estimation** in a classical non parametric fashion applied to `sample` vector. Thus using `gaussian_kde`. Notice `gaussian_kde` includes automatic bandwidth determination.

Finally we plot **mixt** $$f$$ and **kernel** $$\hat f_{kde}$$ using `pl.plot` for a windows plot vector `x`.

As we want to compare our *quantum* way of estimating the density, we have to start out by defining a theoretical baseline model we want to approximate, in our case is `mixt`. Finally, the estimation method we would like to compare our model with is non pararmetric kernel density estimation, so we save this estimation in `kernel`.

```python
from scipy.stats import norm, bernoulli, gaussian_kde

class Mixture(object):

    def __init__(self, loc1=0, scale1=1, loc2=1, scale2=1, alpha=0.5):
        self.var1 = norm(loc=loc1, scale=scale1)
        self.var2 = norm(loc=loc2, scale=scale2)
        self.alpha = alpha
        
    def pdf(self, x):
        return (self.alpha * self.var1.pdf(x) + 
                (1 - self.alpha) * self.var2.pdf(x))

    def rvs(self, size=1):
        vals = np.stack([self.var1.rvs(size), self.var2.rvs(size)],  axis=-1)
        print(vals.shape)
        idx = bernoulli.rvs(1. - self.alpha, size=size)
        return np.array([vals[i, idx[i]] for i in range(size)])

n_var = norm(loc=1, scale=1)
mixt = Mixture(loc1=0, scale1=0.5, loc2=3, scale2=1, alpha=0.5)
sample = mixt.rvs(100) 
kernel = gaussian_kde(sample)
x = np.linspace(-10.,10.,99)
pl.plot(x, mixt.pdf(x), 'r-',  alpha=0.6, label='norm pdf')
pl.plot(x, kernel(x), 'b-',  alpha=0.6, label='kde pdf')

```

### Sample from the Guassian Mixture Model

First we stack the two random samples from the two gaussian densities in `valstest`.


```python
valstest = np.stack([norm(loc = 0, scale = 1).rvs(size = 4),norm(loc = 10, scale = 1).rvs(size = 4)], axis = -1 )
```


```python
valstest
```

    array([[-0.14509451,  9.85286915],
           [ 0.34545315, 11.22459855],
           [-0.38473518,  9.34825703],
           [-0.88421728,  8.82330827]])

Now we generate the indexes that will help us to choose the component according to `alpha` and the `size` of the sample we want (comes from the last step). Notice that we have two components in the mixture, so our vector will contain either `0` or `1`. We recall that here we can use a bernoulli since it has two possible outcomes, but if we´d had N components to sample from we would use a categorical distribution in this step.


```python
idxtest = bernoulli.rvs(1. - 0.5, size=4)
idxtest
```




    array([0, 1, 1, 1])



Now we choose our desired sample from `valstest` according to the index in `idxtest`.


```python
np.array([valstest[i, idxtest[i]] for i in range(4)])
```




    array([-0.14509451, 11.22459855,  9.34825703,  8.82330827])



## Kernel density estimation using Density Matrices

---



### Some context on the layers to be used in `QMDensity()`

To run the `dmkde` method for estimating the density $f$ we do some things first.

Let´s remember that we generated a sample from the gaussian mixture model and save it to `sample`. `sample` shape is `(3,)` so we first reshape it to `(3,1)` in order to have a sample with the correct dimensions into our model.

Define a dimension variable `dim = 300` which will be the number of random Fourier features.

Now we create an object feature map for values in the `x` dimension into the variable `fm_x`. Let´s remember that the class `QFeatureMapRFF` is inherited from `tf.keras.Layer` whose initialization parameters are

        input_dim: dimension of the input
        dim: int. Number of dimensions to represent a sample.
        gamma: float. Gamma parameter of the RBF kernel to be approximated.
        random_state: random number generator seed.

And let's recall that in order to generate random fourier features we must 


0. Choose the number of Random Fourier Features $$R$$. 
1. Draw $R$ samples $$\{\mathbf{w}_1,\dots,\mathbf{w}_R\}$$ from a **gaussian distribution** $$\mathcal{N}(\mathbf{0},\mathbf{I_d})$$ since Bochner theorem asserts that $$K(\Delta) = \int exp(iw\Delta)p(w)dw$$ and we know that under the Fourier transform, the Gaussian function is mapped to another Gaussian function with a different width.
2. Draw $$R$$ samples $$\{b_1,\dots,b_R\}$$ from an uniform distribution in $$(0,2\pi)$$.
3. Our desired random map will be $$\phi_{rff}: \mathbb{R}^d \rightarrow \mathbb{R}^R$$ such that
$$
\phi_{rff}(\mathbf{x}) = \begin{pmatrix} \frac{1}{\sqrt{R}} \sqrt{2}cos(\mathbf{w_1^Tx }+b) \\ \vdots \\ \frac{1}{\sqrt{R}} \sqrt{2}cos(\mathbf{w_R^Tx }+b)\end{pmatrix} 
$$

Notice that in order to build such map we need to do the following matching between math elements and variables in the constructor 

- `input_dim` : $$d$$
- `dim` : $$R$$
- `gamma` : Bandwidth parameter of the internal gaussian distribution.

Notice that at the end with end up with $$\psi = $$ `norm (tf.cos(w^tx +b)* tf.sqrt(2. / self.dim) ) / tf.expand_dims(norm(vector), axis=-1)`.

Understand what `RBFSampler` does ? estimates the internal kernel ? Or the outside kernel ? Why to define such a big layer when you have a direct method that approximates the kernel using `RBFSampler`.

### How does `QFeatureMapRFF` class work ?

Behind the functioning of this layer is the RBF sampler which approximates a Radial Basis Function (the gaussian belongs to this class). From instancing and `RBFSampler` we extract the weights $\mathbf{w_i}$'s and the offsets $b_i$'s and build the Quantum Feature Map.


```python
# d = 1  
from sklearn.kernel_approximation import RBFSampler

rbf_sampler = RBFSampler(
            gamma=1,
            n_components=300,
            random_state=17)
x = np.zeros(shape=(1, 1))
rbf_sampler.fit(x)
rbf_sampler.random_weights_.shape
rbf_sampler.random_offset_.shape
```




    (300,)



What is doing the layer when passing a vector of 300


```python
fm_x = layers.QFeatureMapRFF(1, dim=300, gamma = 1, random_state=17)
fm_x(np.zeros(3).reshape(-1,1)).shape
```




    TensorShape([3, 300])



### How does `QMeasureDensity` layer class work ? 

In this class we compute the value of the density by doing:

$$
\rho = \frac{1}{N} \sum_i^Nz_iz_i^*
$$

where $$N$$ is the number of samples.

### Training a `QMDensity` model 

Here we just use the `QMDensity` model which has the following structure:

```
class QMDensity(tf.keras.Model):
    """
    A Quantum Measurement Density Estimation model.
    Arguments:
        fm_x: Quantum feature map layer for inputs
        dim_x: dimension of the input quantum feature map
    """
    def __init__(self, fm_x, dim_x):
        super(QMDensity, self).__init__()
        self.fm_x = fm_x
        self.dim_x = dim_x
        self.qmd = layers.QMeasureDensity(dim_x)
        self.cp = layers.CrossProduct()
        self.num_samples = tf.Variable(
            initial_value=0.,
            trainable=False     
            )

    def call(self, inputs):
        psi_x = self.fm_x(inputs)
        probs = self.qmd(psi_x)
        return probs

    @tf.function
    def call_train(self, x):
        if not self.qmd.built:
            self.call(x)
        psi = self.fm_x(x)
        rho = self.cp([psi, tf.math.conj(psi)])
        num_samples = tf.cast(tf.shape(x)[0], rho.dtype)
        rho = tf.reduce_sum(rho, axis=0)
        self.num_samples.assign_add(num_samples)
        return rho

    def train_step(self, data):
        x = data
        rho = self.call_train(x)
        self.qmd.weights[0].assign_add(rho)
        return {}

    def fit(self, *args, **kwargs):
        result = super(QMDensity, self).fit(*args, **kwargs)
        self.qmd.weights[0].assign(self.qmd.weights[0] / self.num_samples)
        return result

    def get_config(self):
        base_config = super().get_config()
        return {**base_config}

```

In general it has two layers and they are defined in the `__init__`:

- `fm_x` = Quantum feature map which is a layer of `QFeatureMapFRR`.
- `qmd` = Which is a layer that actually do the measurement which is a layer of type `QMeasureDensity(dim_x)`.


These two layers are concatenated and we can see this model's forward pass inside the `call` method.

Then a customization is made at the level of training the model using auxiliary methods such as `call_train`.

```python
X = sample.reshape((-1, 1))
dim = 300
fm_x = layers.QFeatureMapRFF(1, dim=dim, gamma=1, random_state=17)
qmd = models.QMDensity(fm_x, dim)
qmd.compile()
qmd.fit(X, epochs=1)
out = qmd.predict(x.reshape((-1, 1)))
pl.plot(x, mixt.pdf(x), 'r-',  alpha=0.6, label='norm pdf')
pl.plot(x, kernel(x), 'b-',  alpha=0.6, label='kde pdf')
pl.plot(x, out, 'g-',  alpha=0.6, label='qmkde pdf')

```




![png](/img/posts/Part2_DMKDE/DMKDE_40_2.png)

## Further reading

- More information on customization when using `tf` this can be found at [here](https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers).